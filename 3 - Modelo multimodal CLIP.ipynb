{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo multimodal CLIP\n",
    "\n",
    "Modelos multimodais realizam o processamento de dados de múltiplas modalidades. Veremos especificamente sobre o processamento de texto e imagens, que são os chamados *visual language models*. \n",
    "\n",
    "Implementaremos o modelo CLIP, desenvolvido pela OpenAI.\n",
    "\n",
    "Referências:\n",
    "\n",
    "* [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)\n",
    "* [Repositório do Hugginface](https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_clip.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataset import collate_fn, get_dataset\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ds, _ = get_dataset(\"../Graphs/data/images\", \"../Graphs/data/edgelists.json\", split=0)\n",
    "#ds.transform = None\n",
    "dl = DataLoader(ds, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "imgs, texts = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extração de atributos de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TextEncoder\n",
    "\n",
    "text_encoder = TextEncoder()\n",
    "features = text_encoder(texts)\n",
    "features.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extração de atributos de imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import ResNet50_Weights, resnet50\n",
    "\n",
    "image_encoder = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "image_encoder.fc = nn.Identity()\n",
    "features = image_encoder(imgs)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo multimodal\n",
    "\n",
    "Temos um codificador de imagens que extrai 2048 atributos para cada imagem de entrada e um codificador de texto que extrai 768 atributos para cada texto. Criaremos um modelo que calcula a similaridade entre esses atributos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clip(nn.Module):\n",
    "\n",
    "    def __init__(self, image_encoder, text_encoder, img_dim, text_dim,\n",
    "                 temp=2.6592, dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        # Camadas de projeção\n",
    "        self.visual_projection = nn.Linear(img_dim, dim, bias=False)\n",
    "        self.text_projection = nn.Linear(text_dim, dim, bias=False)\n",
    "        # Parâmetro treinável responsável por reescalar os valores de similaridade\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(temp)) \n",
    "\n",
    "    def project_images(self, imgs):\n",
    "        \"\"\"Codifica imagens.\"\"\"\n",
    "\n",
    "        image_embeds = self.image_encoder(imgs)\n",
    "        image_embeds = self.visual_projection(image_embeds)\n",
    "        # Normaliza os valores pela magnitude do vetor, que é a raiz quadrada \n",
    "        # da soma dos valores ao quadrado\n",
    "        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "        return image_embeds\n",
    "    \n",
    "    def project_texts(self, texts):\n",
    "        \"\"\"Codifica textos.\"\"\"\n",
    "\n",
    "        text_embeds = self.text_encoder(texts)\n",
    "        text_embeds = self.text_projection(text_embeds)\n",
    "        text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "        return text_embeds\n",
    "\n",
    "    def forward(self, imgs, texts, return_emb=False):\n",
    "\n",
    "        image_embeds = self.project_images(imgs)\n",
    "        text_embeds = self.project_texts(texts)\n",
    "        \n",
    "        logit_scale = self.logit_scale.exp()\n",
    "\n",
    "        # Similaridade de coseno. Cada linha i dessa matriz representa a \n",
    "        # similaridade entre o texto i e as imagens do batch. O elemento\n",
    "        # (i,i) representa a similaridade entre o texto i e a imagem correta \n",
    "        # que corresponde a esse texto, enquanto que os demais elementos da \n",
    "        # linha correspondem a correspondências incorretas. \n",
    "        # text_embeds: bs x dim\n",
    "        # image_embeds.t(): dim x \n",
    "        # logits_per_text: bs x bs\n",
    "        logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n",
    "\n",
    "        output = logits_per_text\n",
    "        # Opcionalmente, retorna as projeções das imagens e textos\n",
    "        if return_emb:\n",
    "            output += (image_embeds, text_embeds)\n",
    "\n",
    "        return output\n",
    "    \n",
    "def contrastive_loss(logits_per_text):\n",
    "    \"\"\"Calcula a entropia cruzada para cada linha da matriz, considerando\n",
    "    que a \"classe\" correta da linha i é dada pela coluna i.\"\"\"\n",
    "\n",
    "    scores = logits_per_text\n",
    "    targets = torch.arange(len(logits_per_text), device=logits_per_text.device)\n",
    "    loss = nn.functional.cross_entropy(scores, targets)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def clip_loss(logits_per_text):\n",
    "    \"\"\"Queremos que a matriz de similaridade possua valores altos na diagonal,\n",
    "    e valores baixos fora da diagonal. Essa loss também é chamada de InfoNCE.\"\"\"\n",
    "\n",
    "    caption_loss = contrastive_loss(logits_per_text)\n",
    "    image_loss = contrastive_loss(logits_per_text.t())\n",
    "    return (caption_loss + image_loss) / 2.0\n",
    "\n",
    "# Se quisermos evitar de treinar o encoder de texto, podemos desabilitar\n",
    "# os gradientes\n",
    "text_encoder.requires_grad_(False)\n",
    "model = Clip(image_encoder, text_encoder, img_dim=2048, text_dim=768)\n",
    "\n",
    "logits_per_text = model(imgs, texts)\n",
    "loss = clip_loss(logits_per_text)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# exemplo: BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# texts vem do DataLoader como lista de strings\n",
    "enc_texts = tokenizer(list(texts), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# batch de tensores\n",
    "text_batch = enc_texts[\"input_ids\"].to(device)\n",
    "attn_mask = enc_texts[\"attention_mask\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_texts[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(2184)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(model, image, enc_texts, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # imagem\n",
    "        img_emb = model.project_images(image.to(device))\n",
    "        \n",
    "        # textos já tokenizados\n",
    "        text_embs = model.project_texts(enc_texts)\n",
    "        \n",
    "        sim = torch.matmul(img_emb, text_embs.T).squeeze(0)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = imgs.to(device)\n",
    "sims = similarity(model, images, list(texts), device)\n",
    "\n",
    "print(\"Similaridades:\", sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "# primeiras 1000 imagens\n",
    "sims_first1000 = sims[:1000].reshape(-1)   # pega linhas 0–999\n",
    "# últimas 1000 imagens\n",
    "sims_last1000 = sims[-1000:].reshape(-1)   # pega linhas finais\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(sims_first1000.cpu().numpy(), bins=50, color=\"green\", alpha=0.7)\n",
    "plt.title(\"Similaridades - 1000 primeiras imagens\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(sims_last1000.cpu().numpy(), bins=50, color=\"red\", alpha=0.7)\n",
    "plt.title(\"Similaridades - 1000 últimas imagens\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.hist(sims_first1000.cpu().numpy(), bins=50, alpha=0.7, color=\"green\", label=\"1000 primeiras imagens\")\n",
    "plt.hist(sims_last1000.cpu().numpy(), bins=50, alpha=0.7, color=\"red\", label=\"1000 últimas imagens\")\n",
    "\n",
    "plt.xlabel(\"Similaridade coseno\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.title(\"Distribuição de similaridades\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pega a diagonal -> imagem i com texto i\n",
    "positives = sims.diag().cpu().numpy()\n",
    "\n",
    "# pega tudo fora da diagonal -> negativos\n",
    "mask = ~torch.eye(sims.size(0), dtype=bool, device=sims.device)\n",
    "negatives = sims[mask].cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(positives, bins=50, color=\"green\", alpha=0.7)\n",
    "plt.title(\"Similaridades positivas (imagem ↔ texto correto)\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(negatives, bins=50, color=\"red\", alpha=0.7)\n",
    "plt.title(\"Similaridades negativas (imagem ↔ textos errados)\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sims = sims.diag()                          # diagonal -> imagem i com texto i\n",
    "mask = ~torch.eye(sims.size(0), dtype=bool, device=sims.device)\n",
    "neg_sims = sims[mask]                           # off-diagonal -> todos os pares incorretos\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.hist(pos_sims.cpu().numpy(), bins=50, alpha=0.7, color=\"green\", label=\"Positivos\")\n",
    "plt.hist(neg_sims.cpu().numpy(), bins=50, alpha=0.7, color=\"red\", label=\"Negativos\")\n",
    "\n",
    "plt.xlabel(\"Similaridade coseno\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.title(\"Distribuição de similaridades (positivos vs negativos)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../data/checkpoints/clip-model\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    bf16=True if torch.cuda.is_available() else False,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=6,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "ds_train, ds_valid = get_dataset(\"../Graphs/data/images\", \"../Graphs/data/edgelists.json\", split=0)\n",
    "dl = torch.utils.data.DataLoader(ds_train, batch_size=training_args.per_device_train_batch_size, shuffle=True, num_workers=training_args.dataloader_num_workers)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate, weight_decay=training_args.weight_decay)\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(int(training_args.num_train_epochs)):\n",
    "    total_loss = 0\n",
    "    for step, (imgs, texts) in enumerate(dl):\n",
    "        imgs = imgs.to(device)\n",
    "\n",
    "        # Tokeniza os textos do batch\n",
    "        enc_texts = tokenizer(\n",
    "            list(texts), padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # logits = (batch_img, batch_text) → matriz (B, B)\n",
    "        logits = model(imgs, list(texts))\n",
    "\n",
    "        # CLIP loss (InfoNCE)\n",
    "        loss = clip_loss(logits)\n",
    "        loss.backward()\n",
    "\n",
    "        # grad accumulation\n",
    "        if (step + 1) % training_args.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # logging\n",
    "    avg_loss = total_loss / len(dl)\n",
    "    print(f\"Epoch {epoch+1}/{training_args.num_train_epochs}, Loss média: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sims(model, ds, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    all_image_embs = []\n",
    "    all_texts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, texts in ds:\n",
    "            imgs = imgs.to(device).unsqueeze(0)\n",
    "            all_image_embs.append(model.project_images(imgs))\n",
    "            all_texts.append(texts)  # mantém lista de strings\n",
    "\n",
    "        # concatena embeddings de todas as imagens\n",
    "        image_embs = torch.cat(all_image_embs, dim=0)  # (num_images, dim)\n",
    "\n",
    "        # codifica todos os textos\n",
    "        text_embs = model.project_texts(all_texts).to(device)  # (num_texts, dim)\n",
    "\n",
    "        # matriz de similaridade coseno\n",
    "        sims = torch.matmul(image_embs, text_embs.T)  # (num_images, num_texts)\n",
    "\n",
    "    return sims\n",
    "\n",
    "# Calcula similaridades\n",
    "sims = compute_sims(model, ds_train, device=device)\n",
    "\n",
    "# Histograma comparando 1000 primeiras vs 1000 últimas imagens\n",
    "sims_first1000 = sims[:1000].reshape(-1)\n",
    "sims_last1000 = sims[-1000:].reshape(-1)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(sims_first1000.cpu().numpy(), bins=1000, alpha=0.7, color=\"green\", label=\"1000 primeiras imagens\")\n",
    "plt.hist(sims_last1000.cpu().numpy(), bins=1000, alpha=0.7, color=\"red\", label=\"1000 últimas imagens\")\n",
    "\n",
    "plt.xlabel(\"Similaridade coseno\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.title(\"Distribuição de similaridades (modelo treinado)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_first1000 = sims[:1000, :1000].reshape(-1)\n",
    "sims_last1000 = sims[:1000, 1000:].reshape(-1)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(sims_first1000.cpu().numpy(), bins=1000, alpha=0.7, color=\"green\", label=\"1000 primeiras imagens\")\n",
    "plt.hist(sims_last1000.cpu().numpy(), bins=1000, alpha=0.7, color=\"red\", label=\"1000 últimas imagens\")\n",
    "\n",
    "plt.xlabel(\"Similaridade coseno\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.title(\"Distribuição de similaridades (modelo treinado)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sims.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sims[-100].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_img_alltext(model, image, ds, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    all_texts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, texts in ds:\n",
    "            all_texts.append(texts)  # mantém lista de strings\n",
    "\n",
    "        img_emb = model.project_images(image.unsqueeze(0).to(device))\n",
    "\n",
    "        # codifica todos os textos\n",
    "        text_embs = model.project_texts(all_texts).to(device)  # (num_texts, dim)\n",
    "\n",
    "        # matriz de similaridade coseno\n",
    "        sims = torch.matmul(img_emb, text_embs.T)  # (num_images, num_texts)\n",
    "\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, _ = ds[1500]\n",
    "sims2 = sim_img_alltext(model, image, ds, device=device)\n",
    "\n",
    "# Histograma comparando 1000 primeiras vs 1000 últimas imagens\n",
    "sims_first1000 = sims2[:1000].reshape(-1)\n",
    "sims_last1000 = sims2[-1000:].reshape(-1)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(sims_first1000.cpu().numpy(), bins=1000, alpha=0.7, color=\"green\", label=\"1000 primeiras imagens\")\n",
    "plt.hist(sims_last1000.cpu().numpy(), bins=1000, alpha=0.7, color=\"red\", label=\"1000 últimas imagens\")\n",
    "\n",
    "plt.xlabel(\"Similaridade coseno\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.title(\"Distribuição de similaridades (modelo treinado)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sims2[0].cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot classification\n",
    "\n",
    "Como o modelo envolve similaridade entre texto e imagens, é difícil medir a qualidade de forma intuitiva. Uma forma de fazer isso é verificando a capacidade do modelo de ser aplicado em outras tarefas como classificação de imagens. Como o modelo não foi treinado para tal tarefa, isso é chamado de zero-shot accuracy.\n",
    "\n",
    "Vamos implementar uma função simples de acurácia que verifica a capacidade do modelo em classificar as imagens nas classes gato e chachorro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def zero_shot_classification(model, imgs, label_embeds):\n",
    "\n",
    "    # Projeção das imagens\n",
    "    image_embeds = model.project_images(imgs)\n",
    "    # Similaridade entre cada imagem e as palavras 'cat' e 'dog'\n",
    "    scores = torch.matmul(image_embeds, label_embeds.t())\n",
    "    # Índice da classe mais provável\n",
    "    predictions = scores.argmax(dim=1)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "label_embeds = model.project_texts([\"cat\", \"dog\"])\n",
    "predictions = zero_shot_classification(model, imgs, label_embeds)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, texts):\n",
    "    \"\"\"Mede a acurácia do modelo. Esta função estima a classe correta de cada imagem utilizando\n",
    "    as respectivas legendas. O ideal seria utilizar as classes conhecidas do dataset Oxford Pets,\n",
    "    mas isso adicionaria complexidade ao código.\"\"\"\n",
    "    \n",
    "    targets = []\n",
    "    for text in texts:\n",
    "        if \"cat\" in text or \"kitten\" in text:\n",
    "            target = 0\n",
    "        elif \"dog\" in text or \"puppy\" in text:\n",
    "            target = 1\n",
    "        else:\n",
    "            # Classe não reconhecida\n",
    "            target = 2\n",
    "        targets.append(target)\n",
    "    targets = torch.tensor(targets, device=predictions.device)\n",
    "\n",
    "    # Fração das imagens da classe gato (cachorro) que são mais similares à \n",
    "    # palavra 'cat' ('dog')\n",
    "    mask = targets!=2\n",
    "    targets = targets[mask]\n",
    "    predictions = predictions[mask]\n",
    "    acc = (predictions==targets).float().mean()\n",
    "\n",
    "    return acc\n",
    "\n",
    "accuracy(predictions, texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
